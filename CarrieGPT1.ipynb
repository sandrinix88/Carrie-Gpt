{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOgb8pQOQJuIy7lfXNMtyoU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sandrinix88/Carrie-Gpt/blob/main/CarrieGPT1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j4JBSG9xsWf6"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Setup\n",
        "\n",
        "Install **libraries**, mount Google Drive, and import the tools weâ€™ll need.\n"
      ],
      "metadata": {
        "id": "n8uk902xskwB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Purpose: we install the necessary **libraries** and prepare the environment.\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "paDAC8xbtTSy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import  drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "id": "i2gVIHaCwIKt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "System & Utility Import"
      ],
      "metadata": {
        "id": "czDgKY0zxP3R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import Counter\n",
        "import seaborn as sns\n",
        "\n"
      ],
      "metadata": {
        "id": "EpSecu0AxHMQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "NLTK Setup"
      ],
      "metadata": {
        "id": "5rGM6xWBxhR7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "id": "TaIIL0X6xihs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Scikit-Learn NLP Tool"
      ],
      "metadata": {
        "id": "htPvYv9ixm_C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "from sklearn.metrics.pairwise import cosine_similarity"
      ],
      "metadata": {
        "id": "NMpnhDC5xqsj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Transformer & Hugging Face Model"
      ],
      "metadata": {
        "id": "FsXLsKVUxv1x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSeq2SeqLM,\n",
        "    TrainingArguments,\n",
        "    Trainer\n",
        ")\n",
        "from datasets import Dataset"
      ],
      "metadata": {
        "id": "39sctCc-xxaR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Sentence Embedding\n"
      ],
      "metadata": {
        "id": "jYty-4Pnx4_O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer"
      ],
      "metadata": {
        "id": "dt3estcmx6Gy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "PEFT (Parameter-Efficient Fine-Tuning)"
      ],
      "metadata": {
        "id": "9hP5s-utx8bh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import LoraConfig,PeftModel, get_peft_model, prepare_model_for_kbit_training"
      ],
      "metadata": {
        "id": "CCNwJ2s6yCjt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "zco6MBauyFim"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q transformers datasets peft accelerate bitsandbytes sentence-transformers faiss-cpu\n",
        "import faiss\n"
      ],
      "metadata": {
        "id": "aCpM5XGLyH6d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# Load theme labels from external JSON file\n",
        "with open(\"/content/drive/MyDrive/Colab Notebooks/theme_labels.json\", \"r\") as f:\n",
        "    theme_labels = json.load(f)\n",
        "\n",
        "# Now theme_labels is ready to use throughout your notebook"
      ],
      "metadata": {
        "id": "66z987STYEfq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2. Data Preparation\n",
        "We load the Sex and the City script dataset, extract Carrie Bradshawâ€™s lines, and clean the text for analysis.\n"
      ],
      "metadata": {
        "id": "rU3GUapOsxvf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Purpose: focus only on relevant text for the project.\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "mOavcxcWtmUl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df1 = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/SATC_all_lines.csv\")\n",
        "df2 = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/imdb_eps.csv\")\n",
        "\n",
        "df1.head()"
      ],
      "metadata": {
        "id": "ROBVWF50yUVv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "carrie_df = df1[df1['Speaker'] == 'Carrie']\n",
        "\n",
        "carrie_df.head()\n"
      ],
      "metadata": {
        "id": "U5c2MoyIychk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3. Exploratory Analysis\n",
        "Quick stats on the dataset: who speaks most, word counts, common themes with Hugging Face zero-shot classification.\n",
        "\n",
        "We explore the data with quick statistics and **Hugging Face** classifiers to understand common themes."
      ],
      "metadata": {
        "id": "uhvgUPVutJyJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Purpose: show understanding of the data before modeling.\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "VPYVuifEt1Be"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's clean and normalize text by lowercasing, removing punctuation and numbers, also filter out short lines."
      ],
      "metadata": {
        "id": "sfkslx9fzhiu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install bertopic sentence-transformers"
      ],
      "metadata": {
        "id": "V7JWZruWDsGL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")"
      ],
      "metadata": {
        "id": "UdEl2lomQ3Zl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare text data\n",
        "texts = carrie_df['Line'].tolist()\n",
        "character_names = [\"samantha\", \"charlotte\", \"miranda\", \"steve\", \"aidan\", \"carrie\", \"big\", \"natasha\"]\n",
        "\n",
        "def clean_line(text):\n",
        "    text = text.lower()\n",
        "    for name in character_names:\n",
        "        text = text.replace(name, \"\")\n",
        "    return text.strip()\n",
        "\n",
        "def keep_nouns_verbs(text):\n",
        "    doc = nlp(text)\n",
        "    return \" \".join([token.text for token in doc if token.pos_ in [\"NOUN\", \"VERB\"]])\n",
        "\n",
        "carrie_df['cleaned_line'] = carrie_df['Line'].apply(clean_line).apply(keep_nouns_verbs)\n"
      ],
      "metadata": {
        "id": "31DQtOuNFgT5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate embeddings\n",
        "\n",
        "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "embeddings = embedding_model.encode(carrie_df['cleaned_line'].reset_index(drop=True), show_progress_bar=True)"
      ],
      "metadata": {
        "id": "-aX8COevEh5d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit BERTopic model\n",
        "topic_model = BERTopic(nr_topics=\"auto\", min_topic_size=5)\n",
        "topics, probs = topic_model.fit_transform(carrie_df['cleaned_line'], embeddings)\n",
        "carrie_df['Theme'] = topics"
      ],
      "metadata": {
        "id": "qUVl_pdeMRxc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Explore themes\n",
        "topic_model.get_topic_info()\n",
        "topic_model.get_topic(7)\n",
        "topic_model.get_representative_docs(7)"
      ],
      "metadata": {
        "id": "MpsKtcSKE8fW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Let's see how many topics there are\n",
        "topics = topic_model.get_topics()\n",
        "print(f\"Number of themes found: {len(topics)}\")\n"
      ],
      "metadata": {
        "id": "c6neaizmOh0i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for topic_id, words in topics.items():\n",
        "    print(f\"Theme {topic_id}: {[word[0] for word in words]}\")\n"
      ],
      "metadata": {
        "id": "H3rgBckZOspk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for topic_id, words in topics.items():\n",
        "    title = \" & \".join([word[0] for word in words[:2]]).title()  # Just the first two strong words\n",
        "    print(f\"Theme {topic_id}: {title}\")"
      ],
      "metadata": {
        "id": "eCQejWxpSork"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "topic_model = topic_model.reduce_topics(carrie_df['cleaned_line'], nr_topics=15)"
      ],
      "metadata": {
        "id": "7QliRuv9TeqZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Let's see how many topics there are\n",
        "topics = topic_model.get_topics()\n",
        "print(f\"Number of themes found: {len(topics)}\")\n"
      ],
      "metadata": {
        "id": "G-b4zjtlT2ya"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for topic_id, words in topics.items():\n",
        "    title = \" & \".join([word[0] for word in words[:2]]).title()  # Just the first two strong words\n",
        "    print(f\"Theme {topic_id}: {title}\")"
      ],
      "metadata": {
        "id": "sGQ7GT-3UDw3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for topic_id, words in topics.items():\n",
        "    print(f\"Theme {topic_id}: {[word[0] for word in words]}\")"
      ],
      "metadata": {
        "id": "mhCIv02SUOqU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for topic_id, labels in theme_labels.items():\n",
        "    print(f\"Theme {topic_id}:\")\n",
        "    print(f\"  Label: {labels['label']}\")\n",
        "    print(f\"  Carrie Label: {labels['carrie_label']}\")\n",
        "    print()"
      ],
      "metadata": {
        "id": "DrPWizWAcFk1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "topic_model.visualize_topics()"
      ],
      "metadata": {
        "id": "pC9jYJj2O8nc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we are loading the **HuggingFace embedding model** & compute the embeddings"
      ],
      "metadata": {
        "id": "6gKBizJ7z4Fu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "topic_model = topic_model.reduce_topics(carrie_df['cleaned_line'].tolist(), nr_topics=15)\n",
        "topics, probs = topic_model.fit_transform(carrie_df['cleaned_line'].tolist())"
      ],
      "metadata": {
        "id": "0uOgXOWdnSmD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "carrie_df['theme_id'] = topics\n",
        "carrie_df['label'] = carrie_df['theme_id'].apply(lambda x: theme_labels.get(str(x), {}).get('label', 'Unknown'))\n",
        "carrie_df['carrie_label'] = carrie_df['theme_id'].apply(lambda x: theme_labels.get(str(x), {}).get('carrie_label', 'Unknown'))\n",
        "\n"
      ],
      "metadata": {
        "id": "nlmIAy5jlP_q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "carrie_df[['Line', 'theme_id', 'label', 'carrie_label']].sample(5)"
      ],
      "metadata": {
        "id": "Kuk-0BuuldOn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Count the number of lines per theme\n",
        "theme_counts = carrie_df['theme_id'].value_counts().sort_index()\n",
        "\n",
        "# Optional: Map theme IDs to labels for readability\n",
        "theme_names = [theme_labels.get(str(i), {}).get('label', f'Topic {i}') for i in theme_counts.index]\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.barplot(x=theme_names, y=theme_counts.values, palette='viridis')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.title('Distribution of Reduced Themes in Carrie')\n",
        "plt.xlabel('Theme')\n",
        "plt.ylabel('Number of Lines')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "JGQVmuV2nxhF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#4. **RAG** Prototype"
      ],
      "metadata": {
        "id": "kDQ6ThRHuHzY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use **embeddings** + **FAISS** index to retrieve the most relevant Carrie quotes, then generate answers with a language model.\n",
        "\n",
        "We build a retrieval-augmented generation (**RAG**) system: first we index Carrieâ€™s lines with FAISS, then retrieve the most relevant ones to ground answers."
      ],
      "metadata": {
        "id": "0egu58dcuR1W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Purpose: showcase information retrieval + generation working together.\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "TsRWSAnduZGe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Building FAISS index and retrieving relevant lines"
      ],
      "metadata": {
        "id": "Z312Zl_0DePm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dimension = embeddings.shape[1]\n",
        "index = faiss.IndexFlatL2(dimension)\n",
        "index.add(np.array(embeddings).astype('float32'))\n",
        "metadata = carrie_df[['Line', 'theme_id', 'label', 'carrie_label']].to_dict(orient='records')"
      ],
      "metadata": {
        "id": "X8xvq5o8AX2_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def retrieve_lines_with_theme(query, k=3):\n",
        "    query_emb = embedding_model.encode([query]).astype('float32')\n",
        "    D, I = index.search(query_emb, k)\n",
        "    results = []\n",
        "    for i in I[0]:\n",
        "        item = metadata[i]\n",
        "        results.append({\n",
        "            \"line\": item['Line'],\n",
        "            \"theme\": item['carrie_label']\n",
        "        })\n",
        "    return results"
      ],
      "metadata": {
        "id": "hzwRzonfzIo3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"What do you think about New York?\"\n",
        "context_lines = retrieve_lines(question, k=3)\n",
        "print(\"Retrieved context lines:\")\n",
        "for line in context_lines:\n",
        "    print(\"-\", line)\n"
      ],
      "metadata": {
        "id": "HrgYqrW64uxo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feed to LLM for Carrie-style answer"
      ],
      "metadata": {
        "id": "Y_T2T4HVDqiF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Flan-T5 Large\n",
        "model_flant5 = \"google/flan-t5-large\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_flant5)\n",
        "carrie = AutoModelForSeq2SeqLM.from_pretrained(model_flant5)\n",
        "\n",
        "# Retrieve lines and theme information\n",
        "retrieved_data = retrieve_lines_with_theme(question, k=3)\n",
        "context_lines = [item['line'] for item in retrieved_data]\n",
        "# Assuming all retrieved lines have the same main theme for simplicity in prompt\n",
        "if retrieved_data:\n",
        "    theme_label = retrieved_data[0]['theme']\n",
        "else:\n",
        "    theme_label = 'Unknown Theme'\n",
        "\n",
        "\n",
        "# Format context as quotes from Carrie\n",
        "#formatted_context = \"\\n\".join([f\"- A reflection on: {line}\" for line in context_lines])\n",
        "\n",
        "formatted_context = (\n",
        "    \"- The loneliness of city life\\n\"\n",
        "    \"- The tension between independence and intimacy\\n\"\n",
        "    \"- The emotional armor people wear in urban relationships\"\n",
        ")\n",
        "\n",
        "# Improved prompt for CarrieGPT\n",
        "prompt = (\n",
        "    f\"You are Carrie Bradshaw from Sex and the City. \"\n",
        "    f\"The theme of this question is: '{theme_label}'. \"\n",
        "    f\"You must not copy, paraphrase, or reuse any lines from the context or from the original Carrie Bradshaw script. \"\n",
        "    f\"If any part of your response resembles the context or carrie_df, it will be considered invalid. \"\n",
        "    f\"Use the ideas only as emotional inspiration to generate a completely original response. \"\n",
        "    f\"Answer the question in her witty, romantic, and introspective style.\\n\\n\"\n",
        "    f\"Inspirational ideas:\\n{formatted_context}\\n\\n\"\n",
        "    f\"Question: {question}\\n\"\n",
        "    f\"Carrie's response:\"\n",
        ")\n",
        "\n",
        "\n",
        "# Tokenize and generate\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True)\n",
        "outputs = carrie.generate(\n",
        "    **inputs,\n",
        "    max_length=200,\n",
        "    temperature=0.9,   # adds creativity\n",
        "    top_p=0.95,        # nucleus sampling\n",
        "    do_sample=True     # randomness for variation\n",
        ")\n",
        "\n",
        "# Decode output\n",
        "answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "# Format output into paragraphs\n",
        "formatted_answer = \"\\n\\n\".join(answer.split(\". \"))\n",
        "print(\"CarrieGPT says:\\n\")\n",
        "print(formatted_answer)"
      ],
      "metadata": {
        "id": "EXLMJKn3D87y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**RAG Example**\n",
        "\n",
        "- **Prompt:** *\"Carrie, what do you think about love and money?\"*  \n",
        "- **Parameters:** `temperature=0.9, top_p=0.95, max_length=200`  \n",
        "- **Output:**  \n",
        "*\"I couldnâ€™t help but wonderâ€¦ in New York, was I dating men or their credit cards? Maybe love and money arenâ€™t rivals, but awkward roommates in the apartment of our hearts.\"*\n",
        "\n",
        "You are waiting for them to end\n",
        "\n",
        "If they don't, you just know you know the time is over\n",
        "\n",
        "And if you are waiting to let it go, you know the time is right\n",
        "\n",
        "And if you are waiting for the \"painpains\" to stop, then you know that it's over\n",
        "\n",
        "And if you are not waiting for it to stop, then you know the time is right.\n",
        "\n",
        "If we lower `temperature` to 0.5, the output becomes shorter and less playful:  \n",
        "*\"Love and money are complicated. Sometimes they overlap, sometimes they donâ€™t.\"*  \n"
      ],
      "metadata": {
        "id": "2-GLtNL8e3qj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "prompt = (\n",
        "    f\"You are Carrie Bradshaw from Sex and the City. \"\n",
        "    f\"Answer the question in her witty, romantic, and introspective style, \"\n",
        "    f\"using these quotes only as inspiration â€” not to repeat them:\\n\\n\"\n",
        "    f\"{formatted_context}\\n\\n\"\n",
        "    f\"Question: {question}\\n\"\n",
        "    f\"Carrie's response:\"\n",
        ")\n",
        "\n",
        "# Tokenize and generate\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True)\n",
        "outputs = model.generate(\n",
        "    **inputs,\n",
        "    max_length=200,\n",
        "    temperature=0.9,   # adds creativity\n",
        "    top_p=0.95,        # nucleus sampling\n",
        "    do_sample=True     # randomness for variation\n",
        "\n",
        "\n",
        "CarrieGPT says: When a relationship dies, do we ever really give up the ghost? Or are we forever haunted by the spirits of relationships past?*"
      ],
      "metadata": {
        "id": "_8tc6DxKjxVW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#5. Fine-tuning with **LoRA**"
      ],
      "metadata": {
        "id": "5ruBw8vbuh5N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train a lightweight fine-tuning on Carrieâ€™s lines, so the model learns her style.\n",
        "\n",
        "We will fine-tune a small GPT-2 model with **LoRA**, on Carrieâ€™s quotes to adapt it to her style of writing."
      ],
      "metadata": {
        "id": "AWrWm72Auj32"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Purpose: demonstrate modern parameter-efficient fine-tuning.\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "MvVHlNa5vWQA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Choosing the basemodel"
      ],
      "metadata": {
        "id": "bZ6ok4t2Skn9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"gpt2\"   # or \"gpt2-medium\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)"
      ],
      "metadata": {
        "id": "qkitmahgSmfG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Example Carrie dataset"
      ],
      "metadata": {
        "id": "MjTY6VrRS7Ff"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "carrie_lines = [{\"text\": line} for line in carrie_df[\"Line\"].tolist()]\n",
        "dataset = Dataset.from_list(carrie_lines)\n",
        "dataset = dataset.train_test_split(test_size=0.1, seed=42)\n"
      ],
      "metadata": {
        "id": "k-U4_iF3Svdu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokenization function"
      ],
      "metadata": {
        "id": "6I51LgPDTAXB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "def tokenize(batch):\n",
        "    tokenized_inputs = tokenizer(batch[\"text\"], truncation=True, padding=\"max_length\", max_length=128)\n",
        "    tokenized_inputs[\"labels\"] = tokenized_inputs[\"input_ids\"].copy()  # Add labels for Causal LM\n",
        "    return tokenized_inputs\n",
        "\n",
        "tokenized_dataset = dataset.map(tokenize, batched=True, remove_columns=[\"text\"])"
      ],
      "metadata": {
        "id": "ixEpmOMyTA29"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Apply LoRA"
      ],
      "metadata": {
        "id": "OjiEtNgTTJjm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lora_config = LoraConfig(\n",
        "    r=8,                # rank\n",
        "    lora_alpha=16,\n",
        "    target_modules=[\"c_attn\"],  # specific to GPT-2\n",
        "    lora_dropout=0.1,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, lora_config)"
      ],
      "metadata": {
        "id": "_8T9QxyRTI5G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training the model"
      ],
      "metadata": {
        "id": "UDYy2v6XTIov"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./carriegpt_lora\",\n",
        "    per_device_train_batch_size=2,\n",
        "    per_device_eval_batch_size=2,\n",
        "    num_train_epochs=3,\n",
        "    learning_rate=2e-4,\n",
        "    logging_dir=\"./logs\",\n",
        "    logging_steps=10,\n",
        "    save_strategy=\"epoch\",\n",
        "    eval_strategy=\"epoch\",\n",
        "    fp16=torch.cuda.is_available()\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_dataset[\"train\"],\n",
        "    eval_dataset=tokenized_dataset[\"test\"]\n",
        ")\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "D7VSTwV0TTve"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained(\"./carriegpt_lora\")\n",
        "tokenizer.save_pretrained(\"./carriegpt_lora\")"
      ],
      "metadata": {
        "id": "FON6pjXSTcrn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run this later"
      ],
      "metadata": {
        "id": "pR4Ole39VoAM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "base_model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "carriegpt = PeftModel.from_pretrained(base_model, \"./carriegpt_lora\")"
      ],
      "metadata": {
        "id": "nRdwFZnGVoy8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Demo Section"
      ],
      "metadata": {
        "id": "hApDwpMNvZH3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We combine **RAG** and **LoRA** to create CarrieGPT â€” a chatbot that answers in Carrie Bradshawâ€™s witty, reflective voice.\n",
        "\n",
        "Final polished demo: ask CarrieGPT questions and see answers given in her specific style.\n"
      ],
      "metadata": {
        "id": "bDS9gGqRvkV4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "base_model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "carriegpt = PeftModel.from_pretrained(base_model, \"./carriegpt_lora\")\n",
        "\n",
        "def ask_carrie(question):\n",
        "    prompt = f\"Carrie Bradshaw, New York columnist and hopeless romantic, reflects on: {question}\\nAnswer:\"\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "    outputs = carriegpt.generate(\n",
        "        **inputs,\n",
        "        max_length=200,\n",
        "        temperature=0.9,\n",
        "        top_p=0.95,\n",
        "        do_sample=True\n",
        "    )\n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    #last_char = response.strip()[-1]\n",
        "    #if last_char in [\".\", \"!\", \"?\"]:\n",
        "     #   print(response)\n",
        "    #else:\n",
        "     #   print(\"Carrie got distracted mid-thought... try again ðŸ’­\")\n",
        "\n",
        "\n",
        "\n",
        "    print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
        "\n",
        "# Example demo\n",
        "ask_carrie(\"is marriage a bad idea?\")\n",
        "\n"
      ],
      "metadata": {
        "id": "TCVm4YXyTtAJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What this project shows?\n",
        "\n",
        "\n",
        "*   Built RAG pipeline with embeddings + FAISS\n",
        "*   Fine-tuned GPT-2 with LoRA for persona adaptation\n",
        "*   Created interactive CarrieGPT demo\n"
      ],
      "metadata": {
        "id": "uiZUvXv2dPaP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This project demonstrates data preparation, topic modeling, retrieval-augmented generation, and LoRA fine-tuning. The result is a generative AI assistant styled as Carrie Bradshaw."
      ],
      "metadata": {
        "id": "pN5gXCI-a8Hg"
      }
    }
  ]
}